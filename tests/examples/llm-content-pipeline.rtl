# LLM Content Pipeline - Advanced Multi-Model Workflow
# Demonstrates streaming, structured output, and model-specific optimizations

# ============================================================================
# Configuration Management
# ============================================================================

: setup-fast-model
  "gpt-4o-mini" llm-set-model
  {temperature 0.3 max_tokens 500} llm-set-config
;

: setup-creative-model  
  "gpt-4o" llm-set-model
  {temperature 0.8 max_tokens 1000} llm-set-config
;

: setup-analytical-model
  "gpt-4o" llm-set-model  
  {temperature 0.1 max_tokens 1500} llm-set-config
;

# ============================================================================
# Content Generation Pipeline
# ============================================================================

: generate-outline ( topic -- outline )
  setup-fast-model
  "Create a detailed outline for content about: " swap concat
  llm-call
;

: expand-section ( section-title outline -- expanded-content )
  setup-creative-model
  "Based on this outline: " swap concat
  "\n\nExpand this section with engaging content: " rot concat concat
  llm-call
;

: fact-check-content ( content -- verified-content )
  setup-analytical-model
  ["web_search"] llm-tools
  "Fact-check this content and add citations where needed: " swap concat
  llm-call
;

: polish-writing ( content -- polished-content )
  setup-creative-model
  "Polish this content for clarity, flow, and engagement: " swap concat
  llm-call
;

# ============================================================================
# Streaming Content Generation
# ============================================================================

: stream-generate ( prompt -- content )
  setup-creative-model
  "Generate engaging content about: " swap concat
  llm-stream  # This will stream the response in real-time
;

# ============================================================================
# Structured Output Examples  
# ============================================================================

: generate-json-summary ( content -- json-summary )
  setup-analytical-model
  "Summarize this content as JSON with fields: title, key_points (array), 
word_count, tone, target_audience. Content: " swap concat
  
  # JSON schema for structured output
  {
    "type" "object"
    "properties" {
      "title" {"type" "string"}
      "key_points" {"type" "array" "items" {"type" "string"}}
      "word_count" {"type" "integer"}
      "tone" {"type" "string"}
      "target_audience" {"type" "string"}
    }
    "required" ["title" "key_points" "word_count"]
  } llm-json-schema
;

# ============================================================================
# Multi-turn Conversation Pipeline
# ============================================================================

: start-conversation ( initial-prompt -- response-with-id )
  setup-creative-model
  llm-call
  # Extract response ID for multi-turn (implementation detail)
;

: continue-conversation ( follow-up previous-id -- new-response )
  setup-creative-model
  {model "gpt-4o"} rot llm-multi-turn
;

# ============================================================================
# Content Workflow Orchestration
# ============================================================================

: content-pipeline ( topic -- final-content )
  "Starting content pipeline for: " over concat print
  
  # Stage 1: Generate outline
  "Phase 1: Generating outline..." print
  dup generate-outline
  
  # Stage 2: Expand first section (demo)
  "Phase 2: Expanding content..." print
  dup "Introduction" rot expand-section
  
  # Stage 3: Fact-check
  "Phase 3: Fact-checking..." print  
  fact-check-content
  
  # Stage 4: Polish
  "Phase 4: Polishing..." print
  polish-writing
  
  "Content pipeline completed!" print
;

# ============================================================================
# A/B Testing with Different Models
# ============================================================================

: ab-test-content ( prompt -- comparison )
  # Version A: Fast model
  "=== Version A (Fast Model) ===" print
  dup setup-fast-model swap llm-call
  
  # Version B: Creative model  
  "=== Version B (Creative Model) ===" print
  setup-creative-model llm-call
;

# ============================================================================
# Batch Processing Pipeline
# ============================================================================

: process-topic-list ( topic-list -- results )
  # Process multiple topics efficiently
  setup-fast-model
  "Generate brief summaries for these topics: " swap concat
  llm-call
;

# ============================================================================
# Error Recovery and Fallback
# ============================================================================

: robust-generation ( prompt -- content )
  {
    # Try primary model
    setup-creative-model
    llm-call
  } {
    # Fallback to simpler model
    "Primary model failed, using fallback..." print
    setup-fast-model
    llm-call
  } try-catch
;

# ============================================================================
# Advanced Features Demo
# ============================================================================

: demo-advanced-features ( -- )
  # Reasoning mode example
  "Explain quantum entanglement" llm-reasoning print
  
  # Tool usage example  
  ["web_search"] llm-tools
  "Latest developments in AI safety" llm-call print
  
  # Image analysis (if image provided)
  # "base64-image-data" "Describe this image" llm-image print
  
  # Function calling setup
  # "calculator" {"name" "calculate" "description" "Perform calculations"} llm-function
;

# ============================================================================
# Performance Monitoring
# ============================================================================

: timed-generation ( prompt -- content-with-timing )
  current-time-ms
  rot llm-call
  current-time-ms swap - 
  "Generation took " swap concat " ms" concat print
;

# ============================================================================
# Example Usage and Demonstrations
# ============================================================================

# Demo 1: Complete content pipeline
"The future of sustainable transportation" content-pipeline print

print "========================================" print

# Demo 2: Streaming generation
"Write a creative story about space exploration" stream-generate print

print "========================================" print

# Demo 3: Structured output
"Artificial intelligence is transforming healthcare by enabling more accurate 
diagnoses, personalized treatments, and efficient drug discovery processes."
generate-json-summary print

print "========================================" print

# Demo 4: A/B testing
"Explain blockchain technology" ab-test-content

print "========================================" print

# Demo 5: Batch processing
["Machine Learning" "Quantum Computing" "Biotechnology" "Renewable Energy"]
process-topic-list print

print "========================================" print

# Demo 6: Robust generation with fallback
"Create a comprehensive analysis of climate change impacts" robust-generation print

print "========================================" print

# Demo 7: Advanced features
demo-advanced-features

print "========================================" print

# Demo 8: Performance monitoring
"Generate a technical explanation of neural networks" timed-generation print